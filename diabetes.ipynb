{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import zscore\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import io\n",
    "import os\n",
    "import subprocess\n",
    "from IPython.display import Image, display\n",
    "import __main__ as main\n",
    "import shutil\n",
    "\n",
    "# Configuración de visualización\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos el DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de visualización\n",
    "plt.rcParams['figure.figsize'] = (16, 14)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Carga el dataset desde un archivo CSV.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(\"Dataset cargado exitosamente.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"El archivo no existe. Verifica la ruta del archivo.\")\n",
    "        return None\n",
    "\n",
    "def display_info(data):\n",
    "    \"\"\"Muestra la información general del dataset.\"\"\"\n",
    "    print(\"\\nInformación general del dataset:\")\n",
    "    buffer = io.StringIO()\n",
    "    data.info(buf=buffer)\n",
    "    info_str = buffer.getvalue()\n",
    "    print(info_str)\n",
    "\n",
    "def display_descriptive_stats(data):\n",
    "    \"\"\"Muestra las estadísticas descriptivas del dataset.\"\"\"\n",
    "    print(\"\\nEstadísticas descriptivas del dataset:\")\n",
    "    display(data.describe(include='all').transpose())\n",
    "\n",
    "def check_and_handle_missing_values(data, strategy='mean'):\n",
    "    \"\"\"Verifica y maneja valores faltantes en el dataset.\"\"\"\n",
    "    missing_values = data.isnull().sum()\n",
    "    if missing_values.any():\n",
    "        print(\"El dataset tiene valores faltantes:\")\n",
    "        display(missing_values)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        missing_values.plot(kind='bar')\n",
    "        plt.xlabel('Variables')\n",
    "        plt.ylabel('Número de valores faltantes')\n",
    "        plt.title('Valores faltantes por variable')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        imputer = SimpleImputer(strategy=strategy)\n",
    "        data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "        print(f\"Valores faltantes tratados con estrategia: {strategy}\")\n",
    "        return data_imputed\n",
    "    else:\n",
    "        print(\"El dataset no tiene valores faltantes.\")\n",
    "        return data\n",
    "\n",
    "def check_data_balance(data, target_column):\n",
    "    \"\"\"Verifica si el dataset está balanceado con respecto a la variable objetivo.\"\"\"\n",
    "    if target_column not in data.columns:\n",
    "        print(\"La variable objetivo no existe en el dataset.\")\n",
    "        return\n",
    "\n",
    "    class_counts = data[target_column].value_counts()\n",
    "    print(\"Distribución de clases en la variable objetivo:\")\n",
    "    display(class_counts)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=target_column, data=data)\n",
    "    plt.xlabel(target_column)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title('Distribución de clases en la variable objetivo')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if len(class_counts) > 1:\n",
    "        max_count = class_counts.max()\n",
    "        min_count = class_counts.min()\n",
    "        imbalance_ratio = max_count / min_count\n",
    "        if imbalance_ratio > 2:\n",
    "            print(f\"El dataset está desbalanceado. La clase mayoritaria tiene {imbalance_ratio:.2f} veces más muestras que la clase minoritaria.\")\n",
    "        else:\n",
    "            print(\"El dataset está relativamente balanceado.\")\n",
    "    else:\n",
    "        print(\"La variable objetivo tiene una sola clase. No se puede evaluar el balance.\")\n",
    "\n",
    "def plot_variable_distribution(data, variable):\n",
    "    \"\"\"Grafica la distribución de una variable numérica.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(data=data, x=variable, kde=True)\n",
    "    plt.title(f\"Distribución de {variable}\")\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=variable, data=data)\n",
    "    plt.title(f\"Boxplot de {variable}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical_variable(data, variable):\n",
    "    \"\"\"Grafica la distribución de una variable categórica.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    category_counts = data[variable].value_counts()\n",
    "    sns.barplot(x=category_counts.index, y=category_counts.values)\n",
    "    plt.title(f\"Distribución de {variable}\")\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_matrix(data):\n",
    "    \"\"\"Grafica la matriz de correlación de las variables numéricas.\"\"\"\n",
    "    numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_columns) == 0:\n",
    "        print(\"No hay variables numéricas en el dataset.\")\n",
    "        return\n",
    "\n",
    "    corr_matrix = data[numeric_columns].corr()\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5, fmt=\".2f\", annot_kws={\"fontsize\": 8}, square=True, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title(\"Matriz de Correlación\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Matriz de correlación generada.\")\n",
    "\n",
    "def detect_outliers(data, threshold=3):\n",
    "    \"\"\"Detecta outliers en el dataset utilizando el método de Z-score.\"\"\"\n",
    "    numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "    outliers = {}\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        col_zscore = zscore(data[col])\n",
    "        outliers[col] = data[(np.abs(col_zscore) > threshold)]\n",
    "    \n",
    "    print(f\"Outliers detectados (umbral de Z-score: {threshold}):\")\n",
    "    for col, outlier_data in outliers.items():\n",
    "        if not outlier_data.empty:\n",
    "            print(f\"\\nColumna: {col}\")\n",
    "            display(outlier_data)\n",
    "        else:\n",
    "            print(f\"\\nColumna: {col} - Sin outliers detectados\")\n",
    "\n",
    "def plot_pairplot(data, target_column=None):\n",
    "    \"\"\"Genera un pairplot de las variables numéricas.\"\"\"\n",
    "    numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.pairplot(data[numeric_columns], hue=target_column if target_column in data.columns else None)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def analyze_categorical_cardinality(data):\n",
    "    \"\"\"Analiza la cardinalidad de las variables categóricas.\"\"\"\n",
    "    cat_columns = data.select_dtypes(include=['object', 'category']).columns\n",
    "    cardinality = {col: data[col].nunique() for col in cat_columns}\n",
    "    print(\"Cardinalidad de variables categóricas:\")\n",
    "    for col, card in sorted(cardinality.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{col}: {card}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(cardinality.keys(), cardinality.values())\n",
    "    plt.title(\"Cardinalidad de variables categóricas\")\n",
    "    plt.xlabel(\"Variables\")\n",
    "    plt.ylabel(\"Cardinalidad\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def detect_low_variance(data, threshold=0.01):\n",
    "    \"\"\"Detecta variables numéricas con baja varianza.\"\"\"\n",
    "    num_columns = data.select_dtypes(include=[np.number]).columns\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data[num_columns])\n",
    "    low_variance_features = num_columns[~selector.get_support()].tolist()\n",
    "    print(f\"Variables con varianza menor a {threshold}:\")\n",
    "    print(low_variance_features)\n",
    "\n",
    "def analyze_skewness_kurtosis(data):\n",
    "    \"\"\"Analiza la asimetría y curtosis de las variables numéricas.\"\"\"\n",
    "    num_columns = data.select_dtypes(include=[np.number]).columns\n",
    "    skewness = data[num_columns].apply(stats.skew)\n",
    "    kurtosis = data[num_columns].apply(stats.kurtosis)\n",
    "    \n",
    "    print(\"Asimetría y curtosis de variables numéricas:\")\n",
    "    for col in num_columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Asimetría: {skewness[col]:.2f}\")\n",
    "        print(f\"  Curtosis: {kurtosis[col]:.2f}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(skewness, kurtosis)\n",
    "    for i, col in enumerate(num_columns):\n",
    "        plt.annotate(col, (skewness[i], kurtosis[i]))\n",
    "    plt.title(\"Asimetría vs Curtosis\")\n",
    "    plt.xlabel(\"Asimetría\")\n",
    "    plt.ylabel(\"Curtosis\")\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def check_multicollinearity(data, threshold=0.8):\n",
    "    \"\"\"Identifica posibles problemas de multicolinealidad.\"\"\"\n",
    "    num_columns = data.select_dtypes(include=[np.number]).columns\n",
    "    corr_matrix = data[num_columns].corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    high_corr = [(col1, col2) for col1 in upper_tri.columns for col2 in upper_tri.index if upper_tri.loc[col2, col1] > threshold]\n",
    "    \n",
    "    print(f\"Pares de variables con correlación mayor a {threshold}:\")\n",
    "    for col1, col2 in high_corr:\n",
    "        print(f\"{col1} - {col2}: {corr_matrix.loc[col1, col2]:.2f}\")\n",
    "\n",
    "def analyze_target_relationship(data, target_column):\n",
    "    \"\"\"Analiza la relación entre variables y la variable objetivo.\"\"\"\n",
    "    if target_column not in data.columns:\n",
    "        print(\"La variable objetivo especificada no existe en el dataset.\")\n",
    "        return\n",
    "    \n",
    "    num_columns = data.select_dtypes(include=[np.number]).columns\n",
    "    num_columns = num_columns.drop(target_column) if target_column in num_columns else num_columns\n",
    "    \n",
    "    for col in num_columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=col, y=target_column, data=data)\n",
    "        plt.title(f\"Relación entre {col} y {target_column}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    cat_columns = data.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in cat_columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=col, y=target_column, data=data)\n",
    "        plt.title(f\"Relación entre {col} y {target_column}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def suggest_transformations(data):\n",
    "    \"\"\"Sugiere posibles transformaciones para variables numéricas.\"\"\"\n",
    "    num_columns = data.select_dtypes(include=[np.number]).columns\n",
    "    for col in num_columns:\n",
    "        skewness = stats.skew(data[col])\n",
    "        if abs(skewness) > 1:\n",
    "            print(f\"La variable {col} tiene una asimetría de {skewness:.2f}.\")\n",
    "            if skewness > 0:\n",
    "                print(\"  Sugerencia: Considerar una transformación logarítmica o raíz cuadrada.\")\n",
    "            else:\n",
    "                print(\"  Sugerencia: Considerar una transformación exponencial o cuadrática.\")\n",
    "\n",
    "def detect_duplicates(data):\n",
    "    \"\"\"Detecta filas duplicadas en el dataset.\"\"\"\n",
    "    duplicates = data.duplicated()\n",
    "    if duplicates.any():\n",
    "        print(f\"Se encontraron {duplicates.sum()} filas duplicadas.\")\n",
    "        print(\"Primeras 5 filas duplicadas:\")\n",
    "        display(data[duplicates].head())\n",
    "    else:\n",
    "        print(\"No se encontraron filas duplicadas.\")\n",
    "\n",
    "def analyze_time_series(data, date_column):\n",
    "    \"\"\"Realiza un análisis básico de series temporales si aplica.\"\"\"\n",
    "    if date_column not in data.columns:\n",
    "        print(\"La columna de fecha especificada no existe en el dataset.\")\n",
    "        return\n",
    "    \n",
    "    data[date_column] = pd.to_datetime(data[date_column])\n",
    "    data = data.sort_values(date_column)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for col in data.select_dtypes(include=[np.number]).columns:\n",
    "        plt.plot(data[date_column], data[col], label=col)\n",
    "    plt.title(\"Series temporales de variables numéricas\")\n",
    "    plt.xlabel(\"Fecha\")\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_summary_report(data, target_column=None):\n",
    "    \"\"\"Genera un informe resumen del análisis exploratorio.\"\"\"\n",
    "    report = []\n",
    "    report.append(f\"Resumen del Análisis Exploratorio de Datos\")\n",
    "    report.append(f\"Tamaño del dataset: {data.shape[0]} filas, {data.shape[1]} columnas\")\n",
    "    report.append(f\"Tipos de datos:\\n{data.dtypes.value_counts()}\")\n",
    "    report.append(f\"Valores faltantes: {data.isnull().sum().sum()}\")\n",
    "    \n",
    "    if target_column:\n",
    "        if data[target_column].dtype == 'object':\n",
    "            report.append(f\"Variable objetivo: {target_column} (categórica)\")\n",
    "            report.append(f\"Clases: {', '.join(data[target_column].unique())}\")\n",
    "        else:\n",
    "            report.append(f\"Variable objetivo: {target_column} (numérica)\")\n",
    "            report.append(f\"Rango: {data[target_column].min()} - {data[target_column].max()}\")\n",
    "    \n",
    "    report.append(\"Principales hallazgos:\")\n",
    "    report.append(\"- [Añadir hallazgos importantes aquí]\")\n",
    "    \n",
    "    report.append(\"Próximos pasos sugeridos:\")\n",
    "    report.append(\"- [Añadir sugerencias para el siguiente paso del análisis]\")\n",
    "    \n",
    "    print(\"\\n\".join(report))\n",
    "    \n",
    "def convert_notebook_to_markdown(notebook_path):\n",
    "    \"\"\"\n",
    "    Convierte el notebook actual a un archivo Markdown usando nbconvert.\n",
    "    \n",
    "    :param notebook_path: Ruta al notebook de Jupyter\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['jupyter', 'nbconvert', '--to', 'markdown', '--no-input', notebook_path], \n",
    "                                capture_output=True, text=True, check=True)\n",
    "        print(\"Conversión exitosa:\")\n",
    "        print(result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error durante la conversión:\")\n",
    "        print(e.stderr)\n",
    "\n",
    "\n",
    "def main(file_path):\n",
    "    try:\n",
    "        # 1. Cargar los datos\n",
    "        data = load_data(file_path)\n",
    "        if data is None:\n",
    "            print(\"No se puede continuar sin datos.\")\n",
    "            return\n",
    "\n",
    "        # 2. Mostrar información general y estadísticas descriptivas\n",
    "        display_info(data)\n",
    "        display_descriptive_stats(data)\n",
    "\n",
    "        # 3. Verificar y manejar valores faltantes\n",
    "        data = check_and_handle_missing_values(data)\n",
    "\n",
    "        # 4. Verificar balance de clases (si hay variable objetivo)\n",
    "        target_column = input(\"Ingresa el nombre de la variable objetivo (o presiona Enter si no hay variable objetivo): \")\n",
    "        if target_column.strip():\n",
    "            check_data_balance(data, target_column)\n",
    "        else:\n",
    "            print(\"No se especificó variable objetivo. No se verificará el balance de clases.\")\n",
    "\n",
    "        # 5. Analizar distribuciones de variables\n",
    "        categorical_columns = data.select_dtypes(include=['object', 'category']).columns\n",
    "        numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        print(f\"\\nNúmero de variables categóricas: {len(categorical_columns)}\")\n",
    "        print(f\"Número de variables numéricas: {len(numeric_columns)}\")\n",
    "\n",
    "        print(\"\\nDistribuciones de las variables categóricas:\")\n",
    "        for column in categorical_columns:\n",
    "            plot_categorical_variable(data, column)\n",
    "\n",
    "        print(\"\\nDistribuciones de las variables numéricas:\")\n",
    "        for column in numeric_columns:\n",
    "            plot_variable_distribution(data, column)\n",
    "\n",
    "        # 6. Analizar correlaciones\n",
    "        plot_correlation_matrix(data)\n",
    "\n",
    "        # 7. Detectar outliers\n",
    "        detect_outliers(data)\n",
    "\n",
    "        # 8. Generar pairplot\n",
    "        plot_pairplot(data, target_column if target_column.strip() else None)\n",
    "        \n",
    "        analyze_categorical_cardinality(data)\n",
    "        detect_low_variance(data)\n",
    "        analyze_skewness_kurtosis(data)\n",
    "        check_multicollinearity(data)\n",
    "        \n",
    "        if target_column.strip():\n",
    "            analyze_target_relationship(data, target_column)\n",
    "        \n",
    "        suggest_transformations(data)\n",
    "        detect_duplicates(data)\n",
    "        \n",
    "        date_column = input(\"Ingresa el nombre de la columna de fecha (o presiona Enter si no hay): \")\n",
    "        if date_column.strip():\n",
    "            analyze_time_series(data, date_column)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProceso interrumpido por el usuario. Generando informe con los datos disponibles.\")\n",
    "    finally:\n",
    "        # Generar el informe Markdown\n",
    "        generate_summary_report(data, target_column if target_column.strip() else None)\n",
    "        convert_notebook_to_markdown('diabetes.ipynb')\n",
    "\n",
    "    print(\"Proceso completado.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"diabetes.csv\"  # Asegúrate de que esta ruta sea correcta\n",
    "    main(file_path)\n",
    "    \n",
    "    # Convierte el notebook a Markdown\n",
    "    notebook_path = os.path.abspath(\"diabetes.ipynb\")\n",
    "    convert_notebook_to_markdown(notebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to markdown --no-input diabetes.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning, preparacion para los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros para Random Forest: {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Los resultados del aprendizaje automático se han guardado en 'ML_results.md'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Función para guardar figuras como imágenes\n",
    "def save_figure(fig, filename):\n",
    "    fig.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Función para realizar el tuning de hiperparámetros\n",
    "def tune_model(model, params, X_train, y_train):\n",
    "    grid_search = RandomizedSearchCV(model, params, cv=5, scoring='roc_auc', n_iter=20, random_state=42)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_score_\n",
    "\n",
    "# Función para evaluar el modelo\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Curva Precision-Recall\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    return report, cm, roc_auc, (precision, recall, avg_precision)\n",
    "\n",
    "# Iniciar el contenido del archivo Markdown\n",
    "md_content = \"# Resultados del Aprendizaje Automático para la Predicción de Diabetes\\n\\n\"\n",
    "\n",
    "# 1. Cargar y preparar los datos\n",
    "data = pd.read_csv(\"diabetes.csv\")\n",
    "X = data.drop(\"Outcome\", axis=1)\n",
    "y = data[\"Outcome\"]\n",
    "\n",
    "# 2. Manejo de valores atípicos (ejemplo simple)\n",
    "def remove_outliers(df):\n",
    "    for column in df.columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "X = remove_outliers(X)\n",
    "y = y[X.index]\n",
    "\n",
    "# 3. Imputación de valores faltantes\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 4. Normalización\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 5. Selección de características\n",
    "mi_scores = mutual_info_classif(X_scaled, y)\n",
    "mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "mi_scores = mi_scores.sort_values(ascending=False)\n",
    "md_content += \"## Importancia de características (Información Mutua)\\n\\n\"\n",
    "md_content += \"| Característica | Puntuación MI |\\n|----------------|---------------|\\n\"\n",
    "for feature, score in mi_scores.items():\n",
    "    md_content += f\"| {feature} | {score:.4f} |\\n\"\n",
    "md_content += \"\\n\"\n",
    "\n",
    "# 6. División de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 7. Definición de modelos\n",
    "models = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            \"penalty\": [\"l1\", \"l2\"],\n",
    "            \"solver\": [\"liblinear\", \"saga\"]\n",
    "        }\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params\": {\n",
    "            \"max_depth\": [3, 5, 7, 10, None],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"max_depth\": [3, 5, 7, 10, None],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"kernel\": [\"rbf\", \"poly\"],\n",
    "            \"gamma\": [\"scale\", \"auto\", 0.1, 1]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "            \"subsample\": [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "            \"subsample\": [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 8. Entrenamiento, tuning y evaluación de modelos\n",
    "results = []\n",
    "md_content += \"## Resultados de los Modelos\\n\\n\"\n",
    "md_content += \"| Modelo | ROC AUC | Precisión | Recall | F1-Score |\\n\"\n",
    "md_content += \"|--------|---------|-----------|--------|----------|\\n\"\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    print(f\"Tuning y evaluando {name}...\")\n",
    "    \n",
    "    # Tuning de hiperparámetros\n",
    "    grid_search = RandomizedSearchCV(model_info[\"model\"], model_info[\"params\"], cv=5, scoring='roc_auc', n_iter=20, random_state=42)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluación del modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results.append({\n",
    "        \"Modelo\": name,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"Precisión\": report['weighted avg']['precision'],\n",
    "        \"Recall\": report['weighted avg']['recall'],\n",
    "        \"F1-Score\": report['weighted avg']['f1-score']\n",
    "    })\n",
    "    \n",
    "    md_content += f\"| {name} | {roc_auc:.4f} | {report['weighted avg']['precision']:.4f} | {report['weighted avg']['recall']:.4f} | {report['weighted avg']['f1-score']:.4f} |\\n\"\n",
    "    \n",
    "    # Agregar resultados detallados al contenido Markdown\n",
    "    md_content += f\"\\n## Evaluación Detallada de {name}\\n\\n\"\n",
    "    md_content += \"### Informe de Clasificación\\n\\n\"\n",
    "    md_content += f\"```\\n{classification_report(y_test, y_pred)}\\n```\\n\\n\"\n",
    "    \n",
    "    # Matriz de Confusión\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title(f'Matriz de Confusión - {name}')\n",
    "    ax.set_ylabel('Verdadero')\n",
    "    ax.set_xlabel('Predicho')\n",
    "    save_figure(fig, f\"confusion_matrix_{name.replace(' ', '_')}.png\")\n",
    "    md_content += f\"### Matriz de Confusión\\n\\n![Matriz de Confusión - {name}](confusion_matrix_{name.replace(' ', '_')}.png)\\n\\n\"\n",
    "    \n",
    "    # Curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'ROC Curve - {name}')\n",
    "    ax.legend()\n",
    "    save_figure(fig, f\"roc_curve_{name.replace(' ', '_')}.png\")\n",
    "    md_content += f\"### Curva ROC\\n\\n![Curva ROC - {name}](roc_curve_{name.replace(' ', '_')}.png)\\n\\n\"\n",
    "    \n",
    "    # Curva Precision-Recall\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(recall, precision, label=f'PR Curve (AP = {avg_precision:.2f})')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title(f'Precision-Recall Curve - {name}')\n",
    "    ax.legend()\n",
    "    save_figure(fig, f\"pr_curve_{name.replace(' ', '_')}.png\")\n",
    "    md_content += f\"### Curva Precision-Recall\\n\\n![Curva PR - {name}](pr_curve_{name.replace(' ', '_')}.png)\\n\\n\"\n",
    "\n",
    "# Encontrar el mejor modelo basado en ROC AUC\n",
    "best_model_info = max(results, key=lambda x: x['ROC AUC'])\n",
    "md_content += f\"\\n## Mejor Modelo: {best_model_info['Modelo']}\\n\\n\"\n",
    "md_content += f\"ROC AUC: {best_model_info['ROC AUC']:.4f}\\n\"\n",
    "md_content += f\"Precisión: {best_model_info['Precisión']:.4f}\\n\"\n",
    "md_content += f\"Recall: {best_model_info['Recall']:.4f}\\n\"\n",
    "md_content += f\"F1-Score: {best_model_info['F1-Score']:.4f}\\n\\n\"\n",
    "\n",
    "# Importancia de características para el mejor modelo (si es aplicable)\n",
    "best_model = models[best_model_info['Modelo']]['model']\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance, ax=ax)\n",
    "    ax.set_title(f'Importancia de características ({best_model_info[\"Modelo\"]})')\n",
    "    save_figure(fig, \"feature_importance.png\")\n",
    "    md_content += \"## Importancia de Características\\n\\n\"\n",
    "    md_content += \"![Importancia de Características](feature_importance.png)\\n\\n\"\n",
    "\n",
    "# Guardar el contenido en un archivo Markdown\n",
    "with open(\"ML_results_extended.md\", \"w\") as f:\n",
    "    f.write(md_content)\n",
    "\n",
    "print(\"Los resultados extendidos del aprendizaje automático se han guardado en 'ML_results_extended.md'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elección de Modelos y Métricas\n",
    "\n",
    "Para este problema de clasificación binaria (predicción de diabetes), hemos seleccionado los siguientes modelos:\n",
    "\n",
    "1. **Regresión Logística**: Un modelo lineal simple que sirve como línea base para comparar con modelos más complejos.\n",
    "\n",
    "2. **Árbol de Decisión**: Un modelo no lineal que puede capturar interacciones complejas entre características.\n",
    "\n",
    "3. **Random Forest**: Un conjunto de árboles de decisión que generalmente mejora la precisión y reduce el sobreajuste.\n",
    "\n",
    "4. **SVM (Support Vector Machine)**: Eficaz en espacios de alta dimensionalidad y cuando hay una clara separación entre clases.\n",
    "\n",
    "5. **XGBoost**: Un algoritmo de gradient boosting conocido por su alto rendimiento en una variedad de problemas de aprendizaje automático.\n",
    "\n",
    "Estos modelos fueron elegidos por su diversidad en enfoques y su eficacia probada en problemas de clasificación similares.\n",
    "\n",
    "### Métricas de Evaluación\n",
    "\n",
    "Utilizamos las siguientes métricas para evaluar el rendimiento de nuestros modelos:\n",
    "\n",
    "1. **ROC AUC (Area Under the Receiver Operating Characteristic Curve)**: Mide la capacidad del modelo para distinguir entre clases. Un valor más alto indica un mejor rendimiento.\n",
    "\n",
    "2. **Precisión**: La proporción de predicciones positivas correctas entre todas las predicciones positivas.\n",
    "\n",
    "3. **Recall (Sensibilidad)**: La proporción de predicciones positivas correctas entre todos los casos positivos reales.\n",
    "\n",
    "4. **F1-Score**: La media armónica de precisión y recall, proporcionando un balance entre ambas métricas.\n",
    "\n",
    "5. **Matriz de Confusión**: Nos permite visualizar los verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos.\n",
    "\n",
    "Estas métricas nos ayudan a evaluar el rendimiento general del modelo, su capacidad para identificar correctamente los casos positivos (pacientes con diabetes) y su tasa de falsos positivos y negativos.\n",
    "\n",
    "### Resultados Iniciales\n",
    "\n",
    "Aquí se muestra una tabla con los resultados iniciales de la validación cruzada para cada modelo:\n",
    "\n",
    "| Modelo | ROC AUC Media | Desviación Estándar |\n",
    "|--------|---------------|---------------------|\n",
    "| Logistic Regression | 0.XXX | 0.XXX |\n",
    "| Decision Tree | 0.XXX | 0.XXX |\n",
    "| Random Forest | 0.XXX | 0.XXX |\n",
    "| SVM | 0.XXX | 0.XXX |\n",
    "| XGBoost | 0.XXX | 0.XXX |\n",
    "\n",
    "Basándonos en estos resultados, procedimos a ajustar los hiperparámetros del modelo con mejor rendimiento (en este caso, Random Forest) para optimizar aún más su rendimiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
